# 查找

<!-- toc -->
- [查找](#查找)
  - [二分查找（Binary Search）](#二分查找binary-search)
  - [二叉查找树（Binary Search Tree，BST）](#二叉查找树binary-search-treebst)
    - [二叉查找树的操作及实现](#二叉查找树的操作及实现)
      - [1. 二叉查找树 | 查找（递归算法）](#1-二叉查找树--查找递归算法)
      - [2. 二叉查找树 | 查找（非递归算法，二分）](#2-二叉查找树--查找非递归算法二分)
      - [3. 二叉查找树 | 插入](#3-二叉查找树--插入)
      - [4. 二叉查找树 | 删除](#4-二叉查找树--删除)
    - [二叉查找树查找的性能分析](#二叉查找树查找的性能分析)
  - [AVL树](#avl树)
    - [AVL树的定义](#avl树的定义)
    - [AVL树的关键性质](#avl树的关键性质)
    - [AVL树的旋转](#avl树的旋转)
    - [AVL的树高上界问题](#avl的树高上界问题)
    - [其它主要平衡树](#其它主要平衡树)
  - [散列查找](#散列查找)
    - [散列函数（Hash Function）定义](#散列函数hash-function定义)
    - [散列函数的特点](#散列函数的特点)
    - [常见的散列函数构造方法](#常见的散列函数构造方法)
    - [著名的散列函数](#著名的散列函数)
    - [散列函数的应用场景](#散列函数的应用场景)
    - [散列表（Hash Table）定义](#散列表hash-table定义)
    - [冲突与处理冲突的方法](#冲突与处理冲突的方法)
    - [平均查找长度（ASL）](#平均查找长度asl)

## 二分查找（Binary Search）

二分查找是一种在**有序序列**中查找特定元素的高效算法。它的核心思想是将搜索范围每次减半。这种方法的时间复杂度是\(O(\log n)\)，其中\(n\)是序列中元素的数量。

**原理**：

- 首先，比较序列中间的元素与目标值。
- 如果中间元素正是目标值，则搜索结束。
- 如果目标值小于中间元素，则在左半部分继续搜索。
- 如果目标值大于中间元素，则在右半部分继续搜索。

## 二叉查找树（Binary Search Tree，BST）

二叉查找树是一种特殊的二叉树，它为二分查找提供了数据结构基础。

**特性**：

- 如果结点的左子树非空，则左子树上所有结点的数据小于该结点的数据。
- 如果结点的右子树非空，则右子树上所有结点的数据大于该结点的数据。
- 左、右子树本身也是二叉查找树。

**操作**：

- **查找**：从根结点开始，如果目标数据小于当前结点的数据，继续在左子树查找；如果大于，则在右子树查找。查找的时间复杂度平均是\(O(\log n)\)。
- **插入**：从根结点开始，根据插入值与当前结点的比较结果决定是向左子树还是右子树移动，直到找到合适的插入位置。最坏情况下，插入操作的时间复杂度可能达到\(O(n)\)。
- **删除**：删除操作比较复杂，可能需要考虑多种情况。最坏情况下，删除操作的时间复杂度也是\(O(n)\)。

**应用局限性**：

- 当树变得不平衡时，即某一侧特别长而另一侧较短，二叉查找树的效率会降低，接近线性查找。
- 在进行大量插入或删除操作时，二叉查找树可能需要重新平衡。

为了解决这些局限性，后来又发展了一些平衡的二叉树结构，例如AVL树和红黑树。这些结构可以在插入和删除操作后自动保持或恢复平衡，从而保证操作的高效性。

### 二叉查找树的操作及实现

#### 1. 二叉查找树 | 查找（递归算法）

**步骤**：

1. 检查当前节点是否为空或是否为查找的值，如果是，则返回当前节点。
2. 如果查找的值小于当前节点的值，递归地在左子树中查找。
3. 如果查找的值大于当前节点的值，递归地在右子树中查找。

#### 2. 二叉查找树 | 查找（非递归算法，二分）

**步骤**：

1. 从根节点开始，循环进行搜索。
2. 如果当前节点为空，说明没有找到，返回null。
3. 比较当前节点的值与查找的值。
4. 如果查找的值小于当前节点的值，移动到左子节点。
5. 如果查找的值大于当前节点的值，移动到右子节点。
6. 如果找到了目标值，返回当前节点。

#### 3. 二叉查找树 | 插入

**步骤**：

1. 如果根节点为空，新的节点将成为根节点。
2. 从根节点开始，比较插入的值与当前节点的值。
3. 如果插入的值小于当前节点的值，且左子节点为空，则插入左子节点，否则继续在左子树中递归搜索插入位置。
4. 如果插入的值大于当前节点的值，且右子节点为空，则插入右子节点，否则继续在右子树中递归搜索插入位置。

#### 4. 二叉查找树 | 删除

**步骤**：

1. 被删除的节点是叶子节点：
对于叶子节点，我们只需要将其父节点指向它的指针设置为`nullptr`即可。

1. 被删除的节点只有一个子节点（左子节点或右子节点）：
对于只有一个子节点的节点，我们需要将这个子节点提升到被删除节点的位置。具体来说，我们需要将被删除节点的父节点的相应指针指向被删除节点的子节点。

1. 被删除的节点既有左子节点，也有右子节点：
这是最复杂的情况。通常的处理方法是找到被删除节点的右子树中的最小节点（即右子树的最左边的节点），将其提升到被删除节点的位置，同时处理这个最小节点原来的位置（因为它已经被移动了）。这个最小节点是被删除节点的中序后继节点。***选择前驱（左子树中的最大节点）其实也可以***

***以上所有BST基本操作的代码实现见`Basic Operation of BST.cpp`文件。***

### 二叉查找树查找的性能分析

当我们讨论二叉排序树（Binary Search Tree, BST）的查找性能时，通常会用平均搜索长度（Average Search Length, ASL）来度量。ASL 是对树中节点进行查找操作所需比较次数的平均值。

对于一棵特定的BST，ASL的计算涉及以下几个因素：

1. **树的形状**：树的高度和结构直接影响查找性能。在最理想的情况下，树是完全平衡的，每个节点都有两个子节点（除了叶子节点），这时查找性能最好，ASL接近于树的高度的对数（即 \( \log_2 n \)，其中 \( n \) 是节点的数量）。然而，在最坏的情况下，树可能会退化成一个链表，这时ASL将接近于 \( n \)。

2. **关键字的分布**：关键字在树中的分布也会影响ASL。如果某些关键字被查找的频率更高，那么这些关键字在树的位置会更加影响ASL。

3. **查找顺序**：由于查找顺序的不同，即使是相同的一组关键字，也可能构造出形态各异的多棵BST。每棵树的ASL可能会有很大差异。

要精确计算一棵特定BST的ASL，我们需要考虑树中每个节点的深度（从根到该节点的路径长度），ASL是所有节点的深度之和除以节点总数，其计算公式为：

\[ \text{ASL} = \frac{\sum_{i=1}^{n} \text{深度}(节点_i)}{n} \]

其中 \( n \) 是树中节点的总数，\(\text{深度}(节点_i)\) 是第 \( i \) 个节点的深度。

在实际应用中，为了获得更好的查找性能，我们常常会使用各种平衡树（如AVL树、红黑树）来代替普通的BST，因为平衡树能够在插入和删除操作后自动保持树的平衡，从而保证查找操作的效率。

## AVL树

### AVL树的定义

一种在计算机科学中广泛使用的自平衡二叉查找树（Self-Balancing Binary Search Tree）。它的主要特点是任何节点的左右子树的高度差不会超过1。这个特性使得AVL树在查找、插入和删除操作中都能维持较高的性能，具体体现在这些操作的时间复杂度都是 \(O(log(n))\)，其中 \(n\) 是树中节点的数量。

### AVL树的关键性质

1. **自平衡性**：AVL树的每个节点的左右子树高度差（又称平衡因子：左子树高度 - 右子树高度）的绝对值不超过1。这意味着AVL树会随着数据的插入和删除自动调整结构，保持平衡状态。

2. **旋转操作**：为了维持平衡，AVL树在插入或删除节点后可能需要进行一次或多次的旋转操作。旋转操作有几种类型，如单旋转和双旋转，这些操作帮助树重新获得平衡。

3. **查找效率**：由于AVL树的高度大约是其节点数的对数，因此查找效率很高。

4. **动态操作效率**：在AVL树中插入和删除节点也能保持较高效率，因为每次操作后，树都会通过旋转来重新平衡，确保操作的时间复杂度保持在 \(O(log(n))\)。

### AVL树的旋转

AVL树的旋转操作是为了在插入或删除节点后恢复其平衡性质的关键步骤。通常有四种基本的旋转操作：单向右旋（LL）、单向左旋（RR）、先左后右旋转（LR）和先右后左旋转（RL）。

1. **单向右旋（LL旋转）**:
   - 场景：在节点的左子树的左子树插入一个新节点，导致不平衡。
   - 操作：将不平衡节点向右旋转。
   - 过程：设不平衡节点为A，其左子节点为B。将B变为这部分子树的根节点，A变为B的右子节点，B原来的右子树变为A的左子树。

2. **单向左旋（RR旋转）**:
   - 场景：在节点的右子树的右子树插入一个新节点，导致不平衡。
   - 操作：将不平衡节点向左旋转。
   - 过程：设不平衡节点为A，其右子节点为B。将B变为这部分子树的根节点，A变为B的左子节点，B原来的左子树变为A的右子树。

3. **先左后右旋转（LR旋转）**:
   - 场景：在节点的左子树的右子树插入一个新节点，导致不平衡。
   - 操作：先对不平衡节点的左子节点进行左旋转，然后对不平衡节点进行右旋转。
   - 过程：设不平衡节点为A，其左子节点为B，B的右子节点为C。首先对B进行左旋转，然后对A进行右旋转。

4. **先右后左旋转（RL旋转）**:
   - 场景：在节点的右子树的左子树插入一个新节点，导致不平衡。
   - 操作：先对不平衡节点的右子节点进行右旋转，然后对不平衡节点进行左旋转。
   - 过程：设不平衡节点为A，其右子节点为B，B的左子节点为C。首先对B进行右旋转，然后对A进行左旋转。

每种旋转操作都是为了使树恢复平衡，它们的核心目的都是重新分配节点，以减少树的高度差。在实际操作中，还需要更新每个节点的高度信息。通过这些旋转操作，AVL树能够在每次插入或删除后迅速恢复平衡，保持较高的搜索效率。

### AVL的树高上界问题

**含n个结点的AVL树的最大高度**：

- AVL树的最大高度可以通过斐波那契数列来估算。对于含有n个节点的AVL树，其最大高度大约是 \(1.44 \log_2{(n+2)}\)。这个估算是基于最坏情况下，每次插入都是在较矮的子树进行，从而使树尽可能高。

**高度为d的AVL树的最小节点数**：

- 对于高度为d的AVL树，其最少节点数可以通过递归关系来计算。设 \(N(h)\) 为高度为h的AVL树的最小节点数，则有：
  - \(N(0) = 1\)（只有根节点）
  - \(N(1) = 2\)（根节点加上一个子节点）
  - 对于任意的 \(h > 1\)，有 \(N(h) = N(h-1) + N(h-2) + 1\)。这是因为要形成高度为h的AVL树，我们需要一个根节点，一个高度为 \(h-1\) 的子树和一个高度为 \(h-2\) 的子树。

### 其它主要平衡树

平衡树是计算机科学中的一个重要概念，主要用于保持数据结构在操作（如插入、删除、搜索）过程中的高效性能。除了AVL树之外，还有几种其他类型的平衡树，它们在不同的应用中有着广泛的用途。

1. **红黑树（Red-Black Tree）**：
   - 红黑树是一种自平衡二叉查找树。它通过确保从根到叶子的最长路径不超过最短路径的两倍，来保持树的大致平衡。
   - 应用：红黑树广泛用于C++标准模板库（STL）中，如 `std::map` 和 `std::set`，以及Java的`HashMap`等数据结构中。它们提供了高效的查找、插入和删除操作。

2. **B树（B-Tree）**：
   - B树是一种平衡的多路搜索树，通常用于具有大量数据的系统中。它可以减少读写磁盘的次数，因为一个节点可以有多个子节点。
   - 应用：B树主要用于数据库系统和文件系统中，作为树形索引的数据结构，如MySQL数据库。

3. **B+树（B+-Tree）**：
   - B+树是B树的变种，其中所有的数据都存在叶子节点中，并且叶子节点通过指针相连，这使得范围查询更为高效。
   - 应用：B+树主要用于磁盘和存储系统中。例如，MySQL数据库引擎InnoDB就使用B+树作为其索引的数据结构，特别适合于处理大量数据的读写操作。

## 散列查找

### 散列函数（Hash Function）定义

散列函数是将关键字（Key）映射到表中一个位置的函数。这个位置称为散列值（Hash Value），用于快速查找和存储数据。散列函数的目标是将一个大的关键字集合映射到一个较小的地址空间上。

### 散列函数的特点

1. 映射性：散列函数将关键字映射到地址空间上的特定地址。
2. 压缩性：由于地址空间通常小于关键字集合，多个不同的关键字可能映射到同一个地址，即发生“冲突”。

### 常见的散列函数构造方法

选择一个好的散列函数是关键，理想的散列函数应**计算简单**，且尽量**减少冲突**。虽然不可能找到一个完全不产生冲突的散列函数，但可以通过设计使冲突尽可能少地产生。下面是一些常见的散列函数构造方法：

1. **除留余数法**：这是最常用的散列函数构造方法。其核心思想是取关键字被某个不大于散列表长度 \( m \) 的数 \( p \) 除后的余数作为散列地址，即 \( H(key) = key \mod p \)。在选择 \( p \) 时，通常选取一个**不大于表长**且**与表长近似的质数**，以减少冲突。

2. **直接散列函数**：这种方法简单直接，适用于关键字已经是非负整数且分布均匀的情况。\( H(key) = key \) 或 \( H(key) = a \times key + b \)（其中 \( a \)、\( b \) 为常数）。这种方法的优点是简单快速，但在关键字分布不均匀时可能导致冲突较多。

3. **平方取中法**：先计算关键字的平方值，然后取平方值的中间几位作为散列地址。这种方法的优点是关键字的每一位都对最终的散列地址有影响，因此即使是分布不均的关键字也能较好地分散。适用于关键字的全部或部分位数相同的情况。

4. **折叠法**：将关键字分割成位数相等的几部分（最后一部分位数可以不同），然后叠加这些部分（舍去高位的进位）。叠加有移位叠加和边界叠加两种方式。这种方法适用于关键字位数较长的情况。

5. **数字分析法**：根据关键字的统计特性来设计散列函数。选择其中分布均匀的若干位作为散列地址。这种方法适用于关键字的某些位数上数字分布较为均匀的情况。

6. **随机数法**：通过一个随机函数将关键字映射到散列地址。这种方法在理论上可以避免冲突，但在实际应用中不太常见，因为随机数生成的计算成本较高，且难以保证映射的均匀性。

### 著名的散列函数

1. **MD4（Message-Digest Algorithm 4）**：
   - **设计者**：Ron Rivest（麻省理工学院），1990年。
   - **特点**：产生128位（16字节）的哈希值。MD4旨在快速且简单，主要用于计算机系统而非手持设备。
   - **安全性**：MD4现已被证明存在安全漏洞，不再建议用于加密安全应用。

2. **MD5（Message-Digest Algorithm 5）**：
   - **设计者**：Ron Rivest，1991年，作为MD4的改进版本。
   - **特点**：同样产生128位的哈希值。MD5比MD4更复杂，计算速度稍慢，初衷是提供更高的安全性。
   - **安全性**：MD5已被证明存在弱点，特别是在抗碰撞性方面。尽管如此，MD5在非安全性要求的场合（如校验文件完整性）仍被广泛使用。

3. **SHA 系列（Secure Hash Algorithm）**：
   - **设计者**：美国国家安全局（NSA），由国家标准与技术研究院（NIST）发布。
   - **特点**：SHA是一个哈希函数族，包括SHA-1、SHA-2等多个变体。SHA-1产生160位的哈希值，比MD5更强抗穷举攻击。
   - **安全性**：SHA-1在设计时参考了MD4，并改进了安全性。但随着计算技术的发展，SHA-1的安全性也受到质疑，被证明在某些方面存在弱点，如强抗碰撞性。现在更推荐使用SHA-2或SHA-3。

在实际应用中，应根据数据安全的要求选择合适的哈希函数。对于高安全性要求的场景，推荐使用SHA-2或SHA-3。而MD5和SHA-1虽然安全性不足，但在一些非安全关键的场合（如快速校验数据完整性）仍然可用。

### 散列函数的应用场景

1. **密码保护**：为保护密码安全，系统会存储密码的哈希值，而不是密码本身。登录时，系统比较输入密码的哈希值与存储的哈希值是否匹配，以此验证密码。

2. **云盘秒传**：云盘通过计算并比较文件的MD5哈希值来快速检测文件是否已存在。如果文件的哈希值在服务器上已有记录，则无需重新上传文件。

3. **文件完整性校验**：为确保文件在传输过程中未被篡改，发送端和接收端会分别计算并比较文件的哈希值。如果两端的哈希值一致，说明文件未被改变。

4. **数字签名**：通过对文件或消息的哈希值进行加密，数字签名帮助验证文件的真实性和来源。接收方解密并比较数字签名中的哈希值和自己计算的哈希值，以确保文件的完整性和真实性。

### 散列表（Hash Table）定义

根据设定的**散列函数**和**处理冲突的方法**，将关键字映射到一个连续的地址空间上，形成的这种数据结构称为散列表。在散列表中，每个地址对应一个或多个关键字。

### 冲突与处理冲突的方法

**冲突（Collision）：**

在散列表（Hash Table）中，理想情况下，每个地址（也就是哈希值）应该对应一个唯一的关键字。这种一一对应的关系可以确保最高效的查找性能。但由于哈希函数的本质是一种“压缩映射”，当处理大量关键字时，不同的关键字可能产生相同的哈希值，这就是所谓的“冲突”（Collision）。

**常见的冲突解决方法有：**

1. **链地址法（Chaining）**：在这种方法中，每个散列地址对应的是一个链表。所有映射到同一散列地址的关键字都会被存储在这个地址对应的链表中。查找时，需要遍历相应地址的链表来找到特定关键字。

2. **开放寻址法（Open Addressing）**：当发生冲突时，开放寻址法会寻找另一个空闲的地址来存储当前关键字。这种方法包括线性探测（Linear Probing）、二次探测（Quadratic Probing）和双重散列（Double Hashing）等策略。

   1. **线性探测（Linear Probing）**：
      - 当发生冲突时，线性探测会检查散列表中的下一个地址。
      - 例如，如果散列函数返回的位置已经被占用，它会检查下一个位置，依此类推，直到找到一个空闲的位置。
      - 这种方法的实现简单，但可能导致“聚集”问题，即连续的位置被占用，增加了后续插入和查找操作的开销。

   2. **二次探测（Quadratic Probing）**：
      - 二次探测在冲突发生时，不是简单地检查下一个位置，而是检查当前位置后（前）的第 \(i^2\) 个位置（其中 \(i\) 是探测次数）。
      - 例如，如果第一次探测发生冲突，它会检查当前位置后的第 1 个位置，如果仍然冲突，下次会检查当前位置前的第 1 个位置，依此类推。
      - 这种方法可以减少聚集问题，但计算上比线性探测复杂一些。

   3. **双重散列（Double Hashing）**：
      - 双重散列使用两个散列函数。当第一个散列函数返回的位置被占用时，会使用第二个散列函数计算一个新的位置。
      - 例如，如果第一个散列函数 \(H_1(key)\) 返回的位置已被占用，它会使用第二个散列函数 \(H_2(key)\) 来确定下一个探测的位置。
      - 双重散列的优点在于它减少了聚集问题，并且相比于前两种方法，提供了更好的随机性和均匀性，减少了冲突的机会。

3. **再散列（Rehashing）**：如果散列表变得太满，可能需要使用一个更大的散列表，并应用新的哈希函数重新存储所有元素。

### 平均查找长度（ASL）

在散列查找中，平均查找长度是衡量散列查找效率的一个重要指标，它分为两种情况：查找成功和查找不成功。

1. **ASL成功（Average Search Length for Successful Search）**：
   - 这是指查找到表中已有元素的平均探测次数。
   - 它计算的是找到表中各个已有元素的探测次数的平均值。
   - 公式表示为：\[ \text{ASL成功} = \frac{1}{n} \sum_{i=1}^{n} c_i \]
   - 其中，\( n \) 是散列表中已存储元素的数量，\( c_i \) 是找到第 \( i \) 个已有元素进行关键码比较的次数。

2. **ASL不成功（Average Search Length for Unsuccessful Search）**：
   - 这是指查找不到待查元素，但找到插入位置的平均探测次数。
   - 它是表中所有可能散列到的位置上要插入新元素时，为找到空位置的探测次数的平均值。
   - 公式表示为：\[ \text{ASL不成功} = \frac{1}{n'} \sum_{j=1}^{n'} s_j \]
   - 其中，\( n' \) 是散列函数能够计算出的散列地址数，\( s_j \) 表示一旦在第 \( j \) 个位置插入新元素(原表中没有)时，要找到空闲位置的探测次数。
